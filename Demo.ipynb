{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pistonball_CleanRL import *\n",
    "import torch\n",
    "from supersuit import color_reduction_v0, frame_stack_v1, resize_v1\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"saved_models\"\n",
    "log_dir = \"training_logs\"\n",
    "model_name = \"model_episode_500.pt\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent_module = \"ADG\"\n",
    "n_pistons = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取日志文件\n",
    "log_path = os.path.join(log_dir, \"training_log.csv\")\n",
    "logs = pd.read_csv(log_path)\n",
    "\n",
    "alpha = 0.1  # 平滑系数，可以根据需要调整\n",
    "logs_smoothed = logs.copy()\n",
    "logs_smoothed['episodic_return_smoothed'] = logs['episodic_return'].ewm(alpha=alpha, adjust=False).mean()\n",
    "logs_smoothed['value_loss_smoothed'] = logs['value_loss'].ewm(alpha=alpha, adjust=False).mean()\n",
    "logs_smoothed['policy_loss_smoothed'] = logs['policy_loss'].ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "# 设置 seaborn 样式\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# 绘制 episodic return 曲线\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=logs_smoothed, x='episode', y='episodic_return_smoothed', label='Episodic Return', color='blue')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episodic Return')\n",
    "plt.title('Episodic Return per Episode')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 绘制 value loss 和 policy loss 曲线\n",
    "plt.figure(figsize=(12, 6))\n",
    "# sns.lineplot(data=logs_smoothed, x='episode', y='value_loss_smoothed', label='Value Loss', color='orange')\n",
    "sns.lineplot(data=logs_smoothed, x='episode', y='policy_loss_smoothed', label='Policy Loss', color='green')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Value Loss and Policy Loss per Episode')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path_PPO = os.path.join(log_dir, \"training_log_PPO.csv\")\n",
    "log_path_ADG = os.path.join(log_dir, \"training_log_ADG4.csv\")\n",
    "log_PPO = pd.read_csv(log_path_PPO)\n",
    "log_ADG = pd.read_csv(log_path_ADG)\n",
    "\n",
    "alpha = 0.1  # 平滑系数，可以根据需要调整\n",
    "log_PPO_smoothed = log_PPO.copy()\n",
    "log_PPO_smoothed['episodic_return_smoothed'] = log_PPO_smoothed['episodic_return'].ewm(alpha=alpha, adjust=False).mean()\n",
    "log_ADG_smoothed = log_ADG.copy()\n",
    "log_ADG_smoothed['episodic_return_smoothed'] = log_ADG_smoothed['episodic_return'].ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "# 设置 seaborn 样式\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# 绘制 episodic return 曲线\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=log_PPO_smoothed, x='episode', y='episodic_return_smoothed', label='PPO', color='blue')\n",
    "sns.lineplot(data=log_ADG_smoothed, x='episode', y='episodic_return_smoothed', label='ADG', color='red')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episodic Return')\n",
    "plt.title('Episodic Return per Episode')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" RENDER THE POLICY \"\"\"\n",
    "env = pistonball_v6.parallel_env(render_mode=\"rgb_array\", continuous=False, n_pistons=n_pistons)\n",
    "env = color_reduction_v0(env)\n",
    "env = resize_v1(env, 64, 64)\n",
    "env = frame_stack_v1(env, stack_size=4)\n",
    "num_actions = env.action_space(env.possible_agents[0]).n\n",
    "render_episodes = 3\n",
    "\n",
    "vedio_filename = \"pistonball_v6.mp4\"\n",
    "vedio_path = os.path.join(log_dir, vedio_filename)\n",
    "vedio_fps = 30\n",
    "vedio_framesize = (env.unwrapped.screen_width, env.unwrapped.screen_height)\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "vedio_out = cv2.VideoWriter(vedio_path, fourcc, vedio_fps, vedio_framesize)\n",
    "\n",
    "if agent_module == \"ADG\":\n",
    "    agent = Agent_ADG(num_actions=num_actions).to(device)\n",
    "else:\n",
    "    agent = Agent(num_actions=num_actions).to(device)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "\n",
    "if model_name:\n",
    "    agent.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "agent.eval()\n",
    "num_agents = len(env.possible_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for episode in range(render_episodes):\n",
    "        obs, infos = env.reset(seed=None)\n",
    "        obs = batchify_obs(obs, device)\n",
    "        terms = [False]\n",
    "        truncs = [False]\n",
    "        while not any(terms) and not any(truncs):\n",
    "            vedio_frame = env.render()\n",
    "            vedio_out.write(vedio_frame)\n",
    "            actions, logprobs, _, values = agent(obs)\n",
    "            obs, rewards, terms, truncs, infos = env.step(unbatchify(actions, env))\n",
    "            obs = batchify_obs(obs, device)\n",
    "            terms = [terms[a] for a in terms]\n",
    "            truncs = [truncs[a] for a in truncs]\n",
    "vedio_out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for episode in range(render_episodes):\n",
    "        obs, infos = env.reset(seed=None)\n",
    "        obs = batchify_obs(obs, device).unsqueeze(1)\n",
    "        terms = [False]\n",
    "        truncs = [False]\n",
    "        while not any(terms) and not any(truncs):\n",
    "            vedio_frame = env.render()\n",
    "            vedio_out.write(vedio_frame)\n",
    "\n",
    "            actions = torch.ones(num_agents + 2, dtype=torch.int).to(device)\n",
    "            depend_actions = torch.full((num_agents, 2), 3, dtype=torch.int).to(device)\n",
    "            for ind in range(num_agents - 1, -1, -1):\n",
    "                depend_actions[ind] = actions[ind+1:ind+3]\n",
    "                actions[ind], _, _, _ = agent(obs[ind], depend_actions[ind].unsqueeze(0))\n",
    "            actions = actions[:-2]            \n",
    "            \n",
    "            obs, rewards, terms, truncs, infos = env.step(unbatchify(actions, env))\n",
    "            obs = batchify_obs(obs, device).unsqueeze(1)\n",
    "\n",
    "            terms = [terms[a] for a in terms]\n",
    "            truncs = [truncs[a] for a in truncs]\n",
    "vedio_out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
