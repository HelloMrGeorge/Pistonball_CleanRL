{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pistonball_CleanRL import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from supersuit import color_reduction_v0, frame_stack_v1, resize_v1\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建保存模型的目录\n",
    "save_dir = \"saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 创建保存日志的目录\n",
    "log_dir = \"training_logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ALGO PARAMS\"\"\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ent_coef = 0.1\n",
    "vf_coef = 0.1\n",
    "clip_coef = 0.1\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "stack_size = 4\n",
    "frame_size = (64, 64)\n",
    "max_cycles = 125\n",
    "total_episodes = 2\n",
    "n_pistons = 10\n",
    "\n",
    "logs = []\n",
    "writer = SummaryWriter(log_dir)\n",
    "# model_name = \"model_episode_300.pt\"\n",
    "model_name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ENV SETUP \"\"\"\n",
    "env = pistonball_v6.parallel_env(\n",
    "    render_mode=\"rgb_array\", continuous=False, max_cycles=max_cycles, n_pistons=n_pistons\n",
    ")\n",
    "env = color_reduction_v0(env)\n",
    "env = resize_v1(env, frame_size[0], frame_size[1])\n",
    "env = frame_stack_v1(env, stack_size=stack_size)\n",
    "num_agents = len(env.possible_agents)\n",
    "num_actions = env.action_space(env.possible_agents[0]).n\n",
    "observation_size = env.observation_space(env.possible_agents[0]).shape\n",
    "\n",
    "\"\"\" LEARNER SETUP \"\"\"\n",
    "# agent = Agent(num_actions=num_actions).to(device)\n",
    "agent = Agent_ADG(num_actions=num_actions).to(device)\n",
    "\n",
    "#读取之前的模型\n",
    "previous_episodes = 0\n",
    "if model_name:\n",
    "    model_path = os.path.join(save_dir, model_name)\n",
    "    agent.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    print(\"Model loaded successfully.\")\n",
    "    pattern = re.compile(r\"model_episode_(\\d+)\\.pt\")\n",
    "    match = pattern.match(model_name)\n",
    "    previous_episodes = int(match.group(1)) if match else 0\n",
    "\n",
    "optimizer = optim.Adam(agent.parameters(), lr=0.001, eps=1e-5)\n",
    "\n",
    "\"\"\" ALGO LOGIC: EPISODE STORAGE\"\"\"\n",
    "end_step = 0\n",
    "total_episodic_return = 0\n",
    "rb_obs = torch.zeros((max_cycles, num_agents, stack_size, *frame_size)).to(device)\n",
    "rb_actions = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "rb_logprobs = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "rb_rewards = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "rb_terms = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "rb_values = torch.zeros((max_cycles, num_agents)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TRAINING LOGIC \"\"\"\n",
    "# train for n number of episodes\n",
    "for episode in range(previous_episodes+1, total_episodes+previous_episodes+1):\n",
    "    # collect an episode\n",
    "    with torch.no_grad():\n",
    "        # collect observations and convert to batch of torch tensors\n",
    "        next_obs, info = env.reset(seed=None)\n",
    "        # reset the episodic return\n",
    "        total_episodic_return = 0\n",
    "\n",
    "        # each episode has num_steps\n",
    "        for step in range(0, max_cycles):\n",
    "            # rollover the observation\n",
    "            obs = batchify_obs(next_obs, device)\n",
    "\n",
    "            # get action from the agent\n",
    "            actions, logprobs, _, values = agent(obs)\n",
    "\n",
    "            # execute the environment and log data\n",
    "            next_obs, rewards, terms, truncs, infos = env.step(\n",
    "                unbatchify(actions, env)\n",
    "            )\n",
    "\n",
    "            # add to episode storage\n",
    "            rb_obs[step] = obs\n",
    "            rb_rewards[step] = batchify(rewards, device)\n",
    "            rb_terms[step] = batchify(terms, device)\n",
    "            rb_actions[step] = actions\n",
    "            rb_logprobs[step] = logprobs\n",
    "            rb_values[step] = values.flatten()\n",
    "\n",
    "            # compute episodic return\n",
    "            total_episodic_return += rb_rewards[step].cpu().numpy()\n",
    "\n",
    "            # if we reach termination or truncation, end\n",
    "            if any([terms[a] for a in terms]) or any([truncs[a] for a in truncs]):\n",
    "                end_step = step\n",
    "                break\n",
    "\n",
    "    # bootstrap value if not done\n",
    "    with torch.no_grad():\n",
    "        rb_advantages = torch.zeros_like(rb_rewards).to(device)\n",
    "        for t in reversed(range(end_step)):\n",
    "            delta = (\n",
    "                rb_rewards[t]\n",
    "                + gamma * rb_values[t + 1] * rb_terms[t + 1]\n",
    "                - rb_values[t]\n",
    "            )\n",
    "            rb_advantages[t] = delta + gamma * gamma * rb_advantages[t + 1]\n",
    "        rb_returns = rb_advantages + rb_values\n",
    "\n",
    "    # convert our episodes to batch of individual transitions\n",
    "    b_obs = torch.flatten(rb_obs[:end_step], start_dim=0, end_dim=1)\n",
    "    b_logprobs = torch.flatten(rb_logprobs[:end_step], start_dim=0, end_dim=1)\n",
    "    b_actions = torch.flatten(rb_actions[:end_step], start_dim=0, end_dim=1)\n",
    "    b_returns = torch.flatten(rb_returns[:end_step], start_dim=0, end_dim=1)\n",
    "    b_values = torch.flatten(rb_values[:end_step], start_dim=0, end_dim=1)\n",
    "    b_advantages = torch.flatten(rb_advantages[:end_step], start_dim=0, end_dim=1)\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_index = np.arange(len(b_obs))\n",
    "    clip_fracs = []\n",
    "    for repeat in range(3):\n",
    "        # shuffle the indices we use to access the data\n",
    "        np.random.shuffle(b_index)\n",
    "        for start in range(0, len(b_obs), batch_size):\n",
    "            # select the indices we want to train on\n",
    "            end = start + batch_size\n",
    "            batch_index = b_index[start:end]\n",
    "\n",
    "            _, newlogprob, entropy, value = agent(\n",
    "                b_obs[batch_index], b_actions.long()[batch_index]\n",
    "            )\n",
    "            logratio = newlogprob - b_logprobs[batch_index]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clip_fracs += [\n",
    "                    ((ratio - 1.0).abs() > clip_coef).float().mean().item()\n",
    "                ]\n",
    "\n",
    "            # normalize advantaegs\n",
    "            advantages = b_advantages[batch_index]\n",
    "            advantages = (advantages - advantages.mean()) / (\n",
    "                advantages.std() + 1e-8\n",
    "            )\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -b_advantages[batch_index] * ratio\n",
    "            pg_loss2 = -b_advantages[batch_index] * torch.clamp(\n",
    "                ratio, 1 - clip_coef, 1 + clip_coef\n",
    "            )\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            value = value.flatten()\n",
    "            v_loss_unclipped = (value - b_returns[batch_index]) ** 2\n",
    "            v_clipped = b_values[batch_index] + torch.clamp(\n",
    "                value - b_values[batch_index],\n",
    "                -clip_coef,\n",
    "                clip_coef,\n",
    "            )\n",
    "            v_loss_clipped = (v_clipped - b_returns[batch_index]) ** 2\n",
    "            v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "            v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "    # 记录日志\n",
    "    logs.append({\n",
    "        \"episode\": episode,\n",
    "        \"episodic_return\": np.mean(total_episodic_return),\n",
    "        \"episode_length\": end_step,\n",
    "        \"value_loss\": v_loss.item(),\n",
    "        \"policy_loss\": pg_loss.item(),\n",
    "        \"old_approx_kl\": old_approx_kl.item(),\n",
    "        \"approx_kl\": approx_kl.item(),\n",
    "        \"clip_fraction\": np.mean(clip_fracs),\n",
    "        \"explained_variance\": explained_var.item()\n",
    "    })\n",
    "\n",
    "    writer.add_scalar('Loss/Value Loss', v_loss.item(), episode)\n",
    "    writer.add_scalar('Loss/Policy Loss', pg_loss.item(), episode)\n",
    "    writer.add_scalar('Metrics/Episodic Return', np.mean(total_episodic_return), episode)\n",
    "    writer.add_scalar('Metrics/Episode Length', end_step, episode)\n",
    "    writer.add_scalar('Metrics/Old Approx KL', old_approx_kl.item(), episode)\n",
    "    writer.add_scalar('Metrics/Approx KL', approx_kl.item(), episode)\n",
    "    writer.add_scalar('Metrics/Clip Fraction', np.mean(clip_fracs), episode)\n",
    "    writer.add_scalar('Metrics/Explained Variance', explained_var.item(), episode)\n",
    "\n",
    "    print(f\"Training episode {episode}\")\n",
    "    print(f\"Episodic Return: {np.mean(total_episodic_return)}\")\n",
    "    print(f\"Episode Length: {end_step}\")\n",
    "    print(\"\")\n",
    "    print(f\"Value Loss: {v_loss.item()}\")\n",
    "    print(f\"Policy Loss: {pg_loss.item()}\")\n",
    "    print(f\"Old Approx KL: {old_approx_kl.item()}\")\n",
    "    print(f\"Approx KL: {approx_kl.item()}\")\n",
    "    print(f\"Clip Fraction: {np.mean(clip_fracs)}\")\n",
    "    print(f\"Explained Variance: {explained_var.item()}\")\n",
    "    print(\"\\n-------------------------------------------\\n\")\n",
    "\n",
    "# 保存模型参数\n",
    "model_path = os.path.join(save_dir, f\"model_episode_{total_episodes+previous_episodes}.pt\")\n",
    "torch.save(agent.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# 读取之前的日志文件（如果存在），并保存日志\n",
    "log_path = os.path.join(log_dir, \"training_log.csv\")\n",
    "if os.path.exists(log_path):\n",
    "    logs = pd.read_csv(log_path).to_dict(orient='records') + logs\n",
    "pd.DataFrame(logs).to_csv(log_path, index=False)\n",
    "print(f\"Log saved to {log_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
